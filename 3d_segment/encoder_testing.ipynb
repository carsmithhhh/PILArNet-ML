{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd644d87-9423-45f8-b497-70951ab26f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2902a8-90c3-4a0f-a013-1b8e418daf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183fd4d6-fc7c-4190-b0f3-b2d7a40454b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sdf/home/c/carsmith/PILArNet-ML/3d_segment'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7ea27b-3b6e-4adf-9fa7-58a7139753b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /sdf/home/c/carsmith/PILArNet-ML/3d_segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.PILArNet:[rank: 0] self.emin=0.01, self.emax=20.0, self.energy_threshold=0.13, self.remove_low_energy_scatters=True\n",
      "INFO:data.PILArNet:[rank: 0] Building index\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_116800_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_51800_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_60200_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_64400_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_77600_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/generic_v2_67000_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] 437800 point clouds were loaded\n",
      "INFO:data.PILArNet:[rank: 0] 6 files were loaded\n",
      "INFO:data.PILArNet:[rank: 0] self.emin=0.01, self.emax=20.0, self.energy_threshold=0.13, self.remove_low_energy_scatters=True\n",
      "INFO:data.PILArNet:[rank: 0] Building index\n",
      "INFO:data.PILArNet:[rank: 0] File /sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/val/generic_v2_10880_v1.h5 not found, using all points\n",
      "INFO:data.PILArNet:[rank: 0] 10880 point clouds were loaded\n",
      "INFO:data.PILArNet:[rank: 0] 1 files were loaded\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "from data.PILArNet import PILArNetDataModule\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dataset = PILArNetDataModule(\n",
    "    #data_path=\"../pilarnet/train/*.h5\",\n",
    "    #data_path=\"../../pilarnet/train/*.h5\",\n",
    "    data_path=\"/sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/*.h5\",\n",
    "    batch_size=48,  # 24 events per batch\n",
    "    num_workers=0,\n",
    "    dataset_kwargs={\n",
    "        \"emin\": 1.0e-2,  # min energy for log transform\n",
    "        \"emax\": 20.0,  # max energy for log transform\n",
    "        \"energy_threshold\": 0.13,  # remove points with energy < 0.13\n",
    "        \"remove_low_energy_scatters\": True,  # remove low energy scatters (PID=4)\n",
    "        #\"maxlen\": -1,  # max number of events to iterate over\n",
    "        \"maxlen\": 20000, # taking only first 100 events\n",
    "        \"min_points\": 1024, # minimum number of points in an event\n",
    "    },\n",
    ")\n",
    "dataset.setup()\n",
    "\n",
    "# DataLoader\n",
    "train_loader = dataset.train_dataloader()\n",
    "# subset_indices = list(range(100))  # take first 100 samples\n",
    "# train_subset = Subset(dataset.train_dataset, subset_indices)\n",
    "# subset_loader = torch.utils.data.DataLoader(\n",
    "#     train_subset,\n",
    "#     batch_size=48,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "#     collate_fn=dataset.train_dataloader().collate_fn,  # use same collate\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d528949-c87a-4d16-8012-fb1d5434710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a data\n",
    "# for batch in train_loader:\n",
    "#     points = batch['points']\n",
    "#     lengths = batch['lengths']\n",
    "#     break\n",
    "    \n",
    "# # difference - for cifar, data loader does transforms\n",
    "# # transformed_data = [transform(pc) for pc in raw_data]\n",
    "# print(points[0, :, :].shape)\n",
    "# data = points[0, :, :]\n",
    "# transform = compute_train_transform(seed=45)\n",
    "# x1 = transform(data)\n",
    "# x2 = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974b3111-a40a-49fe-970b-ac4405a88da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=[\n",
    "#     go.Scatter3d(\n",
    "#         x=x1[:, 0], y=x1[:, 1], z=x1[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='red'),\n",
    "#         name='x1'\n",
    "#     ),\n",
    "#     go.Scatter3d(\n",
    "#         x=x2[:, 0], y=x2[:, 1], z=x2[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='blue'),\n",
    "#         name='x2'\n",
    "#     ),\n",
    "#     go.Scatter3d(\n",
    "#         x=data[:, 0], y=data[:, 1], z=data[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='orange'),\n",
    "#         name='original'\n",
    "#     )  \n",
    "# ])\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed42c9a3-6b90-42ab-9153-4b88b33c21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 2 transformed views into sparse tensors\n",
    "# for batch in train_loader:\n",
    "#     for pc in batch['points']:\n",
    "#         x1 = transform(pc)\n",
    "#         x2 = transform(pc)\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "# device = 'cuda'\n",
    "# x1 = torch.tensor(x1).to(device)\n",
    "# x2 = torch.tensor(x2).to(device)\n",
    "\n",
    "# coords = [x1[:, :3], x2[:, :3]]  # list of point clouds, each shape (Ni, 3)\n",
    "# feats = [x1[:, 3:], x2[:, 3:]] # list of energies for each point cloud\n",
    "# voxel_size = 0.05 # change to be real\n",
    "\n",
    "# sparse_tensors = []\n",
    "\n",
    "# for i, pc in enumerate(coords):\n",
    "#     quantized_coords = torch.floor(pc / voxel_size).int()\n",
    "    \n",
    "#     # coordinates = ME.utils.batched_coordinates(quantized_coords)\n",
    "#     batch_index = torch.full((quantized_coords.shape[0], 1), i, dtype=torch.int32, device=quantized_coords.device)\n",
    "#     coords_with_batch = torch.cat([batch_index, quantized_coords], dim=1)  # shape (n, 4)\n",
    "    \n",
    "#     sparse_tensor = ME.SparseTensor(\n",
    "#         features=feats[i].float(),           # shape (n, C)\n",
    "#         coordinates=coords_with_batch      # shape (n, 1 + 3)\n",
    "#     )\n",
    "#     sparse_tensors.append(sparse_tensor)\n",
    "\n",
    "# print(f'Input sizes for x1, x2: {sparse_tensors[0].shape, sparse_tensors[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726aa2a2-bd3e-48de-ae39-8a4d1a1dd1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bb4b2a-18ca-4f2f-95b5-4f24a32a4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting raw events: 100%|██████████| 416/416 [00:16<00:00, 25.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# preparing training data - transforming and converting to sparse tensors in new dataloader\n",
    "from data_utils import *\n",
    "\n",
    "transform = compute_train_transform(seed=45)\n",
    "\n",
    "raw_pointclouds = []\n",
    "train_loader = dataset.train_dataloader()\n",
    "for batch in tqdm(train_loader, desc=\"Extracting raw events\"):\n",
    "    for pc in batch[\"points\"]:\n",
    "        raw_pointclouds.append(pc.cpu().numpy())  # store as NumPy arrays or torch tensors\n",
    "\n",
    "simclr_dataset = SimCLRPointCloudDataset(raw_pointclouds, transform, voxel_size=0.01, device=\"cuda\")\n",
    "\n",
    "def simclr_collate(batch):\n",
    "    x1, x2 = zip(*batch)  # each is a list of SparseTensors\n",
    "    return list(x1), list(x2)\n",
    "\n",
    "simclr_loader = torch.utils.data.DataLoader(\n",
    "    simclr_dataset,\n",
    "    #batch_size=16,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    collate_fn=simclr_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5df088-4bd3-40e3-81de-45f6a35594c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing validadtion data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "mnist3d = MNIST3DExtrudedDataset(train=False, depth=3, voxel_size=1.0, device='cuda')\n",
    "\n",
    "val_size = int(0.2 * len(mnist3d))         # 20% for validation\n",
    "train_size = len(mnist3d) - val_size       # 80% for training\n",
    "\n",
    "mnist_train_dataset, mnist_val_dataset = random_split(mnist3d, [train_size, val_size])\n",
    "collate = lambda x: tuple(zip(*x))\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "mnist_val_loader   = DataLoader(mnist_val_dataset,   batch_size=16, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53d99c-f707-4ccc-ac79-e0ed8a6ec395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarsmith\u001b[0m (\u001b[33mcarsmith-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sdf/home/c/carsmith/PILArNet-ML/3d_segment/wandb/run-20250509_090856-opt8qegj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm/runs/opt8qegj' target=\"_blank\">simclr-run-2</a></strong> to <a href='https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm' target=\"_blank\">https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm/runs/opt8qegj' target=\"_blank\">https://wandb.ai/carsmith-stanford-university/simclr_encoder_pretraining_no_norm/runs/opt8qegj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Epoch final | Loss: 88642.5929 | Accuracy: 0.5630\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Epoch final | Loss: 7013.2594 | Accuracy: 0.6150\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:   1%|▏         | 1/78 [00:09<11:37,  9.05s/it, loss=4.6805]"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from loss import *\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "%autosave 120\n",
    "\n",
    "epochs = 3\n",
    "device = 'cuda'\n",
    "\n",
    "# try tracking with wandb\n",
    "wandb.init(\n",
    "    project=\"simclr_encoder_pretraining_no_norm\",\n",
    "    name=\"simclr-run-2\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": 256,\n",
    "        \"lr\": 1e-4,\n",
    "        \"temperature\": 0.07,\n",
    "    }\n",
    ")\n",
    "\n",
    "# instantiate model - currently, out_features in UNet_Encoder constructor is output of projection head\n",
    "model = UNet_Encoder(in_channels=1)\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) # trying out learning rate decay\n",
    "results = {}\n",
    "results['eval_loss'] = []\n",
    "results['eval_top1'] = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"test\")\n",
    "    train_loss = train_unet(model, simclr_loader, optimizer, epoch, epochs)\n",
    "    scheduler.step()\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': train_loss,}, 'checkpoint.pth')\n",
    "    wandb.log({\n",
    "        \"simclr_train_loss\": train_loss,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    \n",
    "    # validation with classification head\n",
    "    classifier = LinearProbe(out_dim=512, num_classes=10) # encoder embeddings have dim=512\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    eval_loss, eval_acc = mnist_validate(model, classifier, mnist_train_loader, mnist_val_loader, criterion, val_optimizer, epochs=1)\n",
    "    wandb.log({\n",
    "        \"mnist_val_loss\": eval_loss,\n",
    "        \"mnist_val_accuracy\": eval_acc,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    results['eval_loss'].append(eval_loss)\n",
    "    results['eval_top1'].append(eval_acc)\n",
    "\n",
    "np.save('results2.npy', results)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca7389-59a8-410c-9719-25b595643c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['eval_loss'])\n",
    "print(results['eval_top1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5208be0-5453-4a16-84a5-7a53c91928ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
