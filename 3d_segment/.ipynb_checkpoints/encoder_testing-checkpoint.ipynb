{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd644d87-9423-45f8-b497-70951ab26f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2902a8-90c3-4a0f-a013-1b8e418daf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MinkowskiEngine as ME\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fd4d6-fc7c-4190-b0f3-b2d7a40454b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ea27b-3b6e-4adf-9fa7-58a7139753b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from data.PILArNet import PILArNetDataModule\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dataset = PILArNetDataModule(\n",
    "    #data_path=\"../pilarnet/train/*.h5\",\n",
    "    #data_path=\"../../pilarnet/train/*.h5\",\n",
    "    data_path=\"/sdf/data/neutrino/carsmith/foundation_models/pilarnet_model/pilarnet/train/*.h5\",\n",
    "    batch_size=48,  # 24 events per batch\n",
    "    num_workers=0,\n",
    "    dataset_kwargs={\n",
    "        \"emin\": 1.0e-2,  # min energy for log transform\n",
    "        \"emax\": 20.0,  # max energy for log transform\n",
    "        \"energy_threshold\": 0.13,  # remove points with energy < 0.13\n",
    "        \"remove_low_energy_scatters\": True,  # remove low energy scatters (PID=4)\n",
    "        #\"maxlen\": -1,  # max number of events to iterate over\n",
    "        \"maxlen\": 20000, # taking only first 100 events\n",
    "        \"min_points\": 1024, # minimum number of points in an event\n",
    "    },\n",
    ")\n",
    "dataset.setup()\n",
    "\n",
    "# DataLoader\n",
    "train_loader = dataset.train_dataloader()\n",
    "# subset_indices = list(range(100))  # take first 100 samples\n",
    "# train_subset = Subset(dataset.train_dataset, subset_indices)\n",
    "# subset_loader = torch.utils.data.DataLoader(\n",
    "#     train_subset,\n",
    "#     batch_size=48,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "#     collate_fn=dataset.train_dataloader().collate_fn,  # use same collate\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d528949-c87a-4d16-8012-fb1d5434710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a data\n",
    "# for batch in train_loader:\n",
    "#     points = batch['points']\n",
    "#     lengths = batch['lengths']\n",
    "#     break\n",
    "    \n",
    "# # difference - for cifar, data loader does transforms\n",
    "# # transformed_data = [transform(pc) for pc in raw_data]\n",
    "# print(points[0, :, :].shape)\n",
    "# data = points[0, :, :]\n",
    "# transform = compute_train_transform(seed=45)\n",
    "# x1 = transform(data)\n",
    "# x2 = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974b3111-a40a-49fe-970b-ac4405a88da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=[\n",
    "#     go.Scatter3d(\n",
    "#         x=x1[:, 0], y=x1[:, 1], z=x1[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='red'),\n",
    "#         name='x1'\n",
    "#     ),\n",
    "#     go.Scatter3d(\n",
    "#         x=x2[:, 0], y=x2[:, 1], z=x2[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='blue'),\n",
    "#         name='x2'\n",
    "#     ),\n",
    "#     go.Scatter3d(\n",
    "#         x=data[:, 0], y=data[:, 1], z=data[:, 2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=5, color='orange'),\n",
    "#         name='original'\n",
    "#     )  \n",
    "# ])\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed42c9a3-6b90-42ab-9153-4b88b33c21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 2 transformed views into sparse tensors\n",
    "# for batch in train_loader:\n",
    "#     for pc in batch['points']:\n",
    "#         x1 = transform(pc)\n",
    "#         x2 = transform(pc)\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "# device = 'cuda'\n",
    "# x1 = torch.tensor(x1).to(device)\n",
    "# x2 = torch.tensor(x2).to(device)\n",
    "\n",
    "# coords = [x1[:, :3], x2[:, :3]]  # list of point clouds, each shape (Ni, 3)\n",
    "# feats = [x1[:, 3:], x2[:, 3:]] # list of energies for each point cloud\n",
    "# voxel_size = 0.05 # change to be real\n",
    "\n",
    "# sparse_tensors = []\n",
    "\n",
    "# for i, pc in enumerate(coords):\n",
    "#     quantized_coords = torch.floor(pc / voxel_size).int()\n",
    "    \n",
    "#     # coordinates = ME.utils.batched_coordinates(quantized_coords)\n",
    "#     batch_index = torch.full((quantized_coords.shape[0], 1), i, dtype=torch.int32, device=quantized_coords.device)\n",
    "#     coords_with_batch = torch.cat([batch_index, quantized_coords], dim=1)  # shape (n, 4)\n",
    "    \n",
    "#     sparse_tensor = ME.SparseTensor(\n",
    "#         features=feats[i].float(),           # shape (n, C)\n",
    "#         coordinates=coords_with_batch      # shape (n, 1 + 3)\n",
    "#     )\n",
    "#     sparse_tensors.append(sparse_tensor)\n",
    "\n",
    "# print(f'Input sizes for x1, x2: {sparse_tensors[0].shape, sparse_tensors[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726aa2a2-bd3e-48de-ae39-8a4d1a1dd1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bb4b2a-18ca-4f2f-95b5-4f24a32a4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting raw events: 100%|██████████| 208/208 [00:08<00:00, 24.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# preparing training data - transforming and converting to sparse tensors in new dataloader\n",
    "from data_utils import *\n",
    "\n",
    "transform = compute_train_transform(seed=45)\n",
    "\n",
    "raw_pointclouds = []\n",
    "train_loader = dataset.train_dataloader()\n",
    "for batch in tqdm(train_loader, desc=\"Extracting raw events\"):\n",
    "    for pc in batch[\"points\"]:\n",
    "        raw_pointclouds.append(pc.cpu().numpy())  # store as NumPy arrays or torch tensors\n",
    "\n",
    "simclr_dataset = SimCLRPointCloudDataset(raw_pointclouds, transform, voxel_size=0.01, device=\"cuda\")\n",
    "\n",
    "def simclr_collate(batch):\n",
    "    x1, x2 = zip(*batch)  # each is a list of SparseTensors\n",
    "    return list(x1), list(x2)\n",
    "\n",
    "simclr_loader = torch.utils.data.DataLoader(\n",
    "    simclr_dataset,\n",
    "    #batch_size=16,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    collate_fn=simclr_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5df088-4bd3-40e3-81de-45f6a35594c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing validadtion data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "mnist3d = MNIST3DExtrudedDataset(train=False, depth=3, voxel_size=1.0, device='cuda')\n",
    "\n",
    "val_size = int(0.2 * len(mnist3d))         # 20% for validation\n",
    "train_size = len(mnist3d) - val_size       # 80% for training\n",
    "\n",
    "mnist_train_dataset, mnist_val_dataset = random_split(mnist3d, [train_size, val_size])\n",
    "collate = lambda x: tuple(zip(*x))\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "mnist_val_loader   = DataLoader(mnist_val_dataset,   batch_size=16, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e587ff-3355-4cb6-8739-3431a8f618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"simclr_encoder_pretraining\",\n",
    "    name=\"simclr-run-1\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 1e-3,\n",
    "        \"temperature\": 0.07,\n",
    "        # Add more config params if you'd like\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae53d99c-f707-4ccc-ac79-e0ed8a6ec395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Epoch final | Loss: 2.1481 | Accuracy: 0.4645\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Epoch final | Loss: 2.1522 | Accuracy: 0.4465\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimclr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss,}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/PILArNet-ML/3d_segment/model.py:136\u001b[0m, in \u001b[0;36mtrain_unet\u001b[0;34m(model, train_loader, optimizer, epoch, epochs, temperature, device)\u001b[0m\n\u001b[1;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m simclr_loss_vectorized(out_left, out_right, temperature)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 136\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    139\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_i_batch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from loss import *\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "%autosave 120\n",
    "\n",
    "epochs = 5\n",
    "device = 'cuda'\n",
    "\n",
    "# try tracking with wandb\n",
    "wandb.init(\n",
    "    project=\"simclr_encoder_pretraining\",\n",
    "    name=\"simclr-run-1\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": 256,\n",
    "        \"lr\": 1e-4,\n",
    "        \"temperature\": 0.07,\n",
    "    }\n",
    ")\n",
    "\n",
    "# instantiate model - currently, out_features in UNet_Encoder constructor is output of projection head\n",
    "model = UNet_Encoder(in_channels=1)\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) # trying out learning rate decay\n",
    "results = {}\n",
    "results['eval_loss'] = []\n",
    "results['eval_top1'] = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"test\")\n",
    "    train_loss = train_unet(model, simclr_loader, optimizer, epoch, epochs)\n",
    "    scheduler.step()\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': train_loss,}, 'checkpoint.pth')\n",
    "    wandb.log({\n",
    "        \"simclr_train_loss\": train_loss,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    \n",
    "    # validation with classification head\n",
    "    classifier = LinearProbe(out_dim=512, num_classes=10) # encoder embeddings have dim=512\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    eval_loss, eval_acc = mnist_validate(model, classifier, mnist_train_loader, mnist_val_loader, criterion, val_optimizer, epochs=1)\n",
    "    wandb.log({\n",
    "        \"mnist_val_loss\": eval_loss,\n",
    "        \"mnist_val_accuracy\": eval_acc,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    results['eval_loss'].append(eval_loss)\n",
    "    results['eval_top1'].append(eval_acc)\n",
    "\n",
    "np.save('results.npy', results)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca7389-59a8-410c-9719-25b595643c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['eval_loss'])\n",
    "print(results['eval_top1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5208be0-5453-4a16-84a5-7a53c91928ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
